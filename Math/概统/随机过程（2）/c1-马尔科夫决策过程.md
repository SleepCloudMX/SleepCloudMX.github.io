<h1 align="center">马尔科夫决策过程</h1>

$$
% 设置
\newcommand{\aneg}[1]{\hspace{-0.75em}&#1&\hspace{-0.75em}}
\newcommand{\aneq}{\aneg{=}}
% 上述指令用于在使用 array 环境时调整等号左右间距
\newcommand{\noeq}{&\hspace{1.3em}}
% 上述指令用于 align 环境中, 类似与 &= 但不显示等号.
\renewcommand{\d}{\displaystyle}

% 字符
\renewcommand{\i}{\mathrm{i}}
\renewcommand{\j}{\mathrm{j}}
\renewcommand{\k}{\mathrm{k}}
\newcommand{\e}{\textup{e}}
\newcommand{\ve}{\varepsilon}
\newcommand{\Beta}{\mathrm{B}}
\newcommand{\omicron}{\mathit{o}}
\newcommand{\Omicron}{\mathit{O}}

% 原本的定义为:
% \newcommand{\cal}[1]{\mathcal{#1}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\renewcommand{\cal}[1]{\mathcal#1}
\renewcommand{\scr}[1]{\mathscr#1}
\renewcommand{\frak}[1]{\mathfrak#1}
\newcommand{\bb}[1]{\mathbb#1}

% 数集
\newcommand{\D}{\mathbb{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\J}{\mathbb{J}}
\newcommand{\K}{\mathbb{K}}
\renewcommand{\L}{\mathbb{L}}

% 上下标
\newcommand{\trans}{^\mathrm{T}}
\newcommand{\inv}{^{-1}}
\newcommand{\madj}[1]{^{\pqty{#1^*}}}	% m 重伴随矩阵
\newcommand{\adj}{^*}
\newcommand{\vector}[1]{\overrightarrow{#1}}
\newcommand{\wavy}[1]{\overset\sim#1}	% \tilde 或 \widetilde 不明显, 容易与 \bar 或 \overline 混淆

% 序列
\newcommand{\ccdots}{\cdot\cdots\cdot}
\newcommand{\oneton}{1,2,\cdots,n}
\newcommand{\oneto}[1]{1,2,\cdots,#1}

\newcommand{\ssto}[3]{#1_1 #3 #1_2 #3 \cdots #3 #1_{#2}}
\newcommand{\ssup}[3]{#1^1 #3 #1^2 #3 \cdots #3 #1^{#2}}
\newcommand{\soneto}[2]{\ssto{#1}{#2}{,}}
\newcommand{\splus}[2]{\ssto{#1}{#2}{+}}

% 括号
\newcommand{\aqty}[1]{\expval{#1}}
\newcommand{\pbqty}[1]{\left(#1\right]}
\newcommand{\bpqty}[1]{\left[#1\right)}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}

% 矩阵宏简写
\newcommand{\bmatrix}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\Bmatrix}[1]{\begin{Bmatrix}#1\end{Bmatrix}}
\newcommand{\vmatrix}[1]{\begin{vmatrix}#1\end{vmatrix}}
\newcommand{\Vmatrix}[1]{\begin{Vmatrix}#1\end{Vmatrix}}

% 常用微分
\newcommand{\dx}{\dd{x}}
\newcommand{\dy}{\dd{y}}
\newcommand{\dz}{\dd{z}}
\newcommand{\dt}{\dd{t}}
\newcommand{\ds}{\dd{s}}
\newcommand{\dr}{\dd{r}}

% 一般的微分
% 如果只使用 \dd{x}\dd{y} 的话, 中间会有多余的间隔.
\newcommand{\df}{\dd}
\newcommand{\ddf}[2]{\,\mathrm{d}#1\mathrm{d}#2}	% 微分形式 differential form
\newcommand{\dddf}[3]{\,\mathrm{d}#1\mathrm{d}#2\mathrm{d}#3}

% 高阶微分
\newcommand{\dxdy}{\ddf{x}{y}}
\newcommand{\dydz}{\ddf{y}{z}}
\newcommand{\dzdx}{\ddf{z}{x}}
\newcommand{\dudv}{\ddf{u}{v}}
\newcommand{\drdt}{\ddf{r}{\theta}}
\newcommand{\dxdydz}{\dddf{x}{y}{z}}

% 矩阵的宏指令
\newcommand{\pmcmn}[3]{\begin{pmatrix}
	#1_{11} & #1_{12} & \cdots & #1_{1#3} \\
	#1_{21} & #1_{22} & \cdots & #1_{n#3} \\
	\vdots & \vdots && \vdots \\
	#1_{#2 1} & #1_{#2 2} & \cdots & #2_{n#3} \\
\end{pmatrix}}

\newcommand{\pmc}[1]{\pmcmn{#1}{n}{n}}
\newcommand{\pvcn}[2]{\begin{pmatrix}
	#1_1 \\ #1_2 \\ \vdots \\ #1_{#2}
\end{pmatrix}}

\newcommand{\pvc}[1]{\pvcn{#1}{n}}
\newcommand{\pto}{\overset{P}{\to}}

% 函数名
\renewcommand{\char}{\operatorname{char}}	% 由于已存在此命令, 不可使用 DeclareMathOperator
\renewcommand{\r}{\operatorname{r}}
\DeclareMathOperator{\st}{s.t.\,}	% 虽然不是函数名, 但用了这个指令就放这儿了.
\DeclareMathOperator{\diag}{diag}	% 不需要定义太多, 一个文件里用到什么定义什么,
\DeclareMathOperator{\Ker}{Ker}		% 毕竟特殊的函数名太多太多了.
\DeclareMathOperator{\Aut}{Aut}		% 便捷与效率的权衡.
\DeclareMathOperator{\Inn}{Inn}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\stab}{stab}
\DeclareMathOperator{\orb}{orb}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\rot}{rot}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Outer}{Outer}
\DeclareMathOperator{\Even}{Even}
\DeclareMathOperator{\Scalar}{Scalar}
\DeclareMathOperator{\Vector}{Vector}
\DeclareMathOperator{\arsh}{arsh}
\DeclareMathOperator{\arch}{arch}
\DeclareMathOperator{\arth}{arth}
\renewcommand{\Re}{\operatorname{Re}}	% 自带 \Re 的效果是 \mathrm{Re}, 前后无空格, 故重写
\renewcommand{\Im}{\operatorname{Im}}
\DeclareMathOperator{\Sa}{Sa}
\DeclareMathOperator{\Si}{Si}

% 运算符
% 可以用 \bigcap, \bigcup, \bigoplus, \bigotimes 替代
\newcommand{\capop}{\displaystyle\mathop\cap\limits}
\newcommand{\cupop}{\displaystyle\mathop\cup\limits}
\newcommand{\oplusop}{\mathop\oplus\limits}
\newcommand{\otimesop}{\mathop\otimes\limits}
\newcommand{\bigoplusop}{\mathop\bigoplus\limits}
\newcommand{\bigotimesop}{\mathop\bigotimes\limits}

% 积分
\newcommand{\dint}{\displaystyle\int}
\newcommand{\inti}{\dint_{-\infty}^{+\infty}}
\newcommand{\intoi}{\dint_0^{+\infty}}

\newcommand{\intl}{\displaystyle\int\limits}
\newcommand{\iintl}{\displaystyle\iint\limits}
\newcommand{\iiintl}{\displaystyle\iiint\limits}

% 求和
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\csum}[1]{\dsum_{#1=1}^\infty}
\newcommand{\nsum}{\csum{n}}
\newcommand{\nsuminf}{\dsum_{n=-\infty}^{+\infty}}
\newcommand{\ksum}{\csum{k}}
\newcommand{\nosum}{\dsum_{n=0}^\infty}
\newcommand{\insum}{\dsum_{i=1}^n}
\newcommand{\knsum}{\dsum_{k=1}^n}

% 求积
\newcommand{\dprod}{\displaystyle\prod}
\newcommand{\nprod}{\dprod_{n=1}^\infty}
\newcommand{\noprod}{\dprod_{n=0}^\infty}
\newcommand{\inprod}{\dprod_{i=1}^n}

% 极限
\newcommand{\liml}{\lim\limits}
\newcommand{\ulim}{\overline\lim\limits_{n\to\infty}}
\newcommand{\dlim}{\underline\lim\limits_{n\to\infty}}
% 注意这里的 d 是 down, 而不是 displaystyle

\newcommand{\xlim}{\lim\limits_{x\to x_0}}
\newcommand{\nlim}{\lim\limits_{n\to\infty}}
\newcommand{\clim}[1]{\lim\limits_{#1\to\infty}}

% 并集
\newcommand{\incup}{\bigcup_{i=1}^n}
\newcommand{\ncup}{\bigcup_{n=1}^\infty}
\newcommand{\icup}{\bigcup_{i=1}^\infty}

% 交集
\newcommand{\incap}{\bigcap_{i=1}^n}
\newcommand{\ncap}{\bigcap_{n=1}^\infty}
\newcommand{\icap}{\bigcap_{i=1}^\infty}

% 差分
\newcommand{\DD}{\Delta}
\newcommand{\DV}[2]{\dfrac{\DD#1}{\DD#2}}
\newcommand{\nDV}[3]{\dfrac{\DD^{#1}#2}{\DD#3^{#1}}}

% 求导
\newcommand{\ddv}{\displaystyle\dv}
\newcommand{\dpdv}{\displaystyle\pdv}

% 最值 (返回参数); 暂时先这么凑合着用吧
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}}

% 缩写
\newcommand{\LRA}{\Leftrightarrow}
\newcommand{\RLA}{\Leftrightarrow}
\newcommand{\LA}{\Leftarrow}
\newcommand{\RA}{\Rightarrow}

\newcommand{\lra}{\leftrightarrow}
\newcommand{\rla}{\leftrightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\ra}{\rightarrow}

\newcommand{\QRLA}{\quad\RLA\quad}
\newcommand{\QRA}{\quad\RA\quad}
\newcommand{\LLRA}{\Longleftrightarrow}

\newcommand{\QNRA}{\quad\nRightarrow\quad}
\newcommand{\qnra}{\quad\nrightarrow\quad}

\newcommand{\wt}{\widetilde}

% 图形符号
\newcommand{\qed}{\quad\square}
\renewcommand{\parallel}{\mathrel{/\mskip-2.5mu/}}
\newcommand{\paralleleq}{\hspace{0.5em}{^{^{\parallel}}}\hspace{-1.04em}=}
\newcommand{\rt}{\matrm{Rt}\triangle}

% 分块矩阵
\newenvironment{mat}[1]{
	\begin{array}{#1}
}{
	\end{array}
}

\newenvironment{pmat}[1]{
	\left( \begin{array}{#1}
}{
	\end{array} \right)
}

\newenvironment{bmat}[1]{
	\left[ \begin{array}{#1}
}{
	\end{array} \right]
}

\newenvironment{Bmat}[1]{
	\left\{ \begin{array}{#1}
}{
	\end{array} \right\}
}

\newenvironment{vmat}[1]{
	\left\lvert \begin{array}{#1}
}{
	\end{array} \right\rvert
}

\newenvironment{Vmat}[1]{
	\left\lVert \begin{array}{#1}
}{
	\end{array} \right\rVert
}
$$

[TOC]

### 1.1  随机过程

#### 1.1.1  概率论的公理化

见笔记：[概率论的公理化](https://sleepcloudmx.github.io/Math/概统/随机过程/c1-概率论公理化.html#11--概率公理化).

<span style="border-left: 4px solid #dfe2e5; padding: 0 15px; color: #777777; padding-right: 0;">了解即可，不要求掌握.</span>

#### 1.1.2  随机过程的概念

见笔记：[随机过程的概念](https://sleepcloudmx.github.io/Math/概统/随机过程/c1-概率论公理化.html#15--随机过程的概念).

<span style="border-left: 4px solid #dfe2e5; padding: 0 15px; color: #777777; padding-right: 0;">了解什么是随机过程即可，概率特征不需要掌握.</span>

#### 1.1.3  随机过程的分类

见笔记：[随机过程的分类](https://sleepcloudmx.github.io/Math/概统/随机过程/c1-概率论公理化.html#16--随机过程的分类).

<span style="border-left: 4px solid #dfe2e5; padding: 0 15px; color: #777777; padding-right: 0;">不看也没关系，独立增量过程、马尔可夫过程、计数过程后面都会重新提到.</span>



### 1.2  马尔科夫过程

#### 1.2.1  基本概念

见笔记：[马尔科夫过程](https://sleepcloudmx.github.io/Math/概统/随机过程/c1-概率论公理化.html#162--马尔可夫过程)、[马尔科夫链](https://sleepcloudmx.github.io/Math/概统/随机过程/c3-马尔科夫链.html#311--马尔科夫链的定义).

**备注**

- 若随机过程 $ \Bqty{X(t), t \in T} $ 满足无后效性, 则称为 **马尔科夫过程**.
  - 若时间空间 $ T $ 为有限集或可列集, 则称为 **马尔科夫链** (如伯努利过程).
  - 若时间空间 $ T $ 为连续集, 则称为 **连续参数马尔科夫链**. (如泊松过程).
  - 若时间空间 $ T = \bpqty{0, +\infty} $, 则称为 **扩散过程** (如布朗运动).
- 对于马尔科夫链, 我们只研究 $ T = \N $ 的情况, 即 $ \Bqty{X_n, n \ge 0} $.
  - **一步转移概率** 为 $ p_{ij}(n) := P\Bqty{X_{n+1} = j \mid X_n = i} $,
  - 若一步转移概率与时间无关, 即 $ p_{ij}(n) = p_{ij} $, 则称为 **齐次马氏链**.
  - 以后我们只研究齐次马氏链的情况, $ \bm P = (p_{ij}) $ 称为 **概率转移矩阵**.
- 对于无后效性的说明,
  - $ P\Bqty{X_{n+1} \mid X_n, X_{n-1}} = P\Bqty{X_{n+1} \mid X_n} $.
  - 可证 $ P\Bqty{X_{n+1} \mid X_n} $ 与 $ P\Bqty{X_{n-1} \mid X_n} $ 独立.
  - 但是 $ P\Bqty{X_{n+1}} $ 与 $ P\Bqty{X_{n-1}} $ 不一定独立.
- 马尔科夫链的状态空间 $ S $ 一般为 $ \N $ 或其子集.



#### 1.2.2  CK 方程

**定义**	称 $ p_{ij}^{(m)} = P\Bqty{X_{n + m} = j \mid X_n = i} $ 为 $m$ 步转移概率, $ \bm P^{(m)} = (p_{ij}^{(m)}) $ 为 $m$ 步转移概率矩阵. 约定 $ p_{ii}^{(0)} = 0 $.

**定理 (Chapman-Kolmogorov 方程)**	
$$
\forall m, n \in \N^+ \colon \bm P^{(m+n)} = \bm P^{(m)} \bm P^{(n)}.
$$

<div style="background-color: #f3f2ee">
    <details>
        <summary><b>证明</b>
        </summary>
        <iframe src="ifsrc\1.2.2 Chapman-Kolmogorov 方程.html" height=250></iframe>
    </details>
</div>

**推论**	$ \forall n \in \N^+ \colon \bm P^{(n)} = \bm P^{(n-1)} \bm P^{(1)} = \pqty{\bm P^{(1)}}^n = \bm P^n $.



#### 1.2.3  极限概率

**定义**	马氏链的概率分布向量定义为
$$
\begin{align}
\pi_i(n) &= P\Bqty{X_n = i}, \\
\bm \pi(n) &= \pqty{
	\pi_1(n), \cdots, \pi_i(n)
}.
\end{align}
$$
并称 $ \bm \pi(0) $ 为马氏链的 **初始分布**.

**备注**	$ \bm \pi(n) = \bm \pi(0) \bm P^n $.

---

**定义**	若定义在 $S$ 上的概率分布 $ \bm \pi = \Bqty{\pi_1, \pi_2, \cdots} $ 满足 $ \bm \pi = \bm \pi \bm P $, 即
$$
\pi_j = \sum_{i \in S} \pi_i p_{ij},
$$
则称为马氏链的 **平稳分布** (或不变概率测度).

**备注**	

- 于是平稳分布满足 $ \bm \pi = \bm \pi \bm P^n $.
- 平稳分布可能不存在, 存在也不一定唯一.

---

**定义**	若 $ \nlim \bm \pi(n) = \bm \pi^* $ 存在, 则称 $ \bm \pi^* $ 为马氏链的 **极限分布**.

**备注**	

- 若 **极限概率** $ \nlim \bm P^n $ 存在, 则极限分布 $ \nlim \bm \pi(n) = \bm \pi(0) \nlim \bm P^n $ 存在.
- 极限分布是平稳分布, 即 $ \bm \pi^* = \nlim \bm \pi(0) \bm P^n = \nlim \bm \pi(0) \bm P^{(n+1)} = \bm \pi^* \bm P %= \bm \pi^* \bm P^n $.
- 平稳分布不一定是极限分布. 极限分布若存在则唯一, 而平稳分布则不然.

---

**定理**	不可约遍历链有如下性质

1. 极限概率存在, 且 $ \nlim p_{ij}^{(n)} $ 与 $i$ 无关.
2. 平稳分布唯一存在, 且 $ \forall i, j \in S \colon \pi_j = \nlim p_{ij}^{(n)} %= \dfrac{1}{\mu_i} $.

**备注**	

- 于是联立 $ \forall j \in S \colon \pi_j = \dsum_{i \in S} \pi_i p_{ij} $ 与 $ \dsum_{i \in S} \pi_i = 1 $ 即可求得极限概率.
- 不可约遍历链的定义、定理的证明、更多的性质可参考[马尔科夫链笔记](https://sleepcloudmx.github.io/Math/概统/随机过程/c3-马尔科夫链.html).



### 1.3  马尔科夫奖励过程

#### 1.3.1  基本概念

这里重新表述马尔科夫过程, 并引入马尔科夫奖励过程:

- 马尔可夫过程 $ \aqty{\cal S, \bm{\cal P}} $.
  - 简称 MP, 即 Markov Process.
  - $ \cal S \subseteq \N $ 是状态空间, $ S_t \in \cal S $ 表示 $t$ 时刻的状态.
  - $ \bm{\cal P} $ 是状态转移概率矩阵, $ \cal P_{ss'} = \bb P\bqty{S_{t+1} = s' \mid S_t = s} $.
  - 状态的 **轨迹** $ \tau = \Bqty{s_1, s_2, \cdots} $ 即固定事件下随机过程的样本.
- 马尔科夫决策过程 $ \aqty{\cal S, \cal P, \cal R, \gamma} $.
  - 简称 MRP, 即 Markov Reward Process.
  - 在 $ \aqty{\cal S, \bm{\cal P}} $ 的基础上, 这里的 $ \cal S $ 是有限集.
  - $ \cal R $ 称为回报或者奖励, $ R_t $ 是 $t$ 时刻的 **立即奖励**.
  - $ \cal R_s = \bb E\bqty{R_{t+1} \mid S_t = s} $ 是状态 $s$ 的奖励函数, 或称 **状态奖励**.



#### 1.3.2  整体回报与值函数

- 整体回报
  - 常数 $ \gamma \in [0, 1] $ 称为 **折扣因子**.
  - **整体回报** $ G_t = \dsum_{k=0}^\infty \gamma^k R_{t + k \color{red}+ 1} $ 又称为长期回报.
  - $ \gamma $ 表示整体回报中未来奖励的权重. 其余解释见 PPT 或教材.
- 值函数 (状态值函数)
  - **值函数** $ v(s) = \bb E\bqty{G_t \mid S_t = s} $ 是整体回报的条件期望.
  - $ v(s) = \bb E\bqty{R_{t+1} + \gamma R_{t+2} + \cdots \mid S_t = s} $. (对回报和轨迹的期望)
  - $ v(s) = \bb E\bqty{\cal R_{S_t} + \gamma \cal R_{S_{t+1}} + \gamma^2 \cal R_{S_{t+2}} + \cdots \mid S_t = s} $.(对轨迹的期望)



#### 1.3.2  贝尔曼方程

- $ v(s) = \bb E\bqty{R_{t + 1} + \gamma v(S_{t + 1}) \mid S_t = s} $.
- $ v(s) = \cal R_s + \gamma \dsum_{s' \in S} \cal P_{ss'} v(s') $.
- $ \begin{bmatrix}
  	v(1) \\ \vdots \\ v(n)
  \end{bmatrix} = \begin{bmatrix}
  	\cal R_1 \\ \vdots \\ \cal R_n
  \end{bmatrix} + \gamma \begin{bmatrix}
  	\cal P_{11} & \cdots & \cal P_{1n} \\
  	\vdots & & \vdots \\
  	\cal P_{11} & \cdots & \cal P_{nn}
  \end{bmatrix} \begin{bmatrix}
  	v(1) \\ \vdots \\ v(n)
  \end{bmatrix} $.
- $ \bm v = \bm{\cal R} + \gamma \bm{\cal P} \bm v = (\bm I - \gamma \bm{\cal P})\inv \bm{\cal R} $.



### 1.4  马尔科夫决策过程

#### 1.4.1  基本概念

马尔科夫决策过程 $ \aqty{\cal S, \cal A, \cal P, \cal R, \gamma} $ 简称 MDP, 即 Markov Decision Process.

在[马尔科夫奖励过程](# 1.3  马尔科夫奖励过程)的基础上, 每次进入一个状态 $ s \in \cal S $ 后, 执行动作 (行为) $ a \in \cal A $ 后进入下一个状态.

- $ \cal A \subset \N $ 是有限的行为集合.
  - $ \cal P_{ss'}^a = P\Bqty{S_{t + 1} = s' \mid S_t = s, A_t = a} $.
  - $ \cal R_s^a = E\Bqty{R_{t + 1} \mid S_t = s, A_t = a} $.
- **策略** $ \pi $ 给出了任一状态下采取行为的概率分布.
  - 这里的策略 $ \pi $ 与之前提到的[初始分布](# 1.2.3  极限概率)等没有任何关系.
  - $ \pi(a \mid s) = P\Bqty{A_t = a \mid S_t = s} $. (概率分布)
  - $ \pi(s) $ 表示在 $ s $ 状态下可能采取的行为的集合.
  - $ \cal P_{ss'}^\pi = \dsum_{a \in \cal A} \pi(a \mid s) \cal P_{ss'}^a $. (即全概率公式)
  - $ \cal R_s^\pi = \dsum_{a \in \cal A} \pi(a \mid s) \cal R_s^a $. (即 $ \bb E\bqty{X} = \bb E\bqty{\bb E\bqty{X \mid Y}} $)



#### 1.4.2  值函数与贝尔曼期望方程

状态值函数的定义与结论马尔科夫奖励过程的相同.

- **状态值函数**
  - $ v_\pi(s) := E_\pi\bqty{G_t \mid S_t = s} $.
  - $ v_\pi(s) = E_\pi\bqty{R_{t+1} + \gamma v_\pi(S_{t + 1}) \mid S_t = s} $.
  - $ \bm v_\pi = \bm{\cal R}^\pi + \gamma \bm{\cal P}^\pi \bm v_\pi = \pqty{\bm I - \gamma \bm{\cal P}^\pi}\inv \bm{\cal R}^\pi $.
- **行为值函数**
  - $ q_\pi(s, a) := E_\pi\bqty{G_t \mid S_t = s, A_t = a} $.
  - $ q_\pi(s, a) = E_\pi\bqty{R_{t + 1} + \gamma q_\pi(S_{t + 1}, A_{t + 1}) \mid S_t = s, A_t = a} $.
- 由 $ \bb E\bqty{X} = \bb E\bqty{\bb E\bqty{X \mid Y}} $ 得到相互关系:
  - $ v_\pi(s) = \dsum_{a \in \cal A} \pi(a \mid s) q_\pi(s, a) $.
  - $ q_\pi(s, a) = \cal R_s^a + \gamma \dsum_{s' \in \cal S} \cal P_{ss'}^a v_\pi(s') $.
- 于是代入即得
  - $ v_\pi(s) = \dsum_{a \in \cal A} \pi(a \mid s) \pqty{
    	\cal R_s^a + \gamma \dsum_{s' \in \cal S}
    	\cal P_{ss'}^a v_\pi(s')
    } $. (不包含行为值函数的方程)
  - $ q_\pi(s, a) = \cal R_s^a +
    \gamma \dsum_{s' \in \cal S} \cal P_{ss'}^a
    \dsum_{a' \in \cal A} \pi(a' \mid s') q_\pi(s', a') $. (不包含状态值函数的方程)

以上除定义之外均称为贝尔曼期望方程, 其中以最后两个公式最为重要.



#### 1.4.3  最优值函数与贝尔曼最优方程

- 最优值函数

  - 最优状态值函数 $ \d v_*(s) := \max\limits_\pi v_\pi(s) $.
  - 最优行为值函数 $ q_*(s, a) := \d\max_\pi q_\pi(s, a) $.

- 相互关系

  - 最优状态值函数 $ v_*(s) = \max\limits_a q_*(s, a) $.
  - 最优行为值函数 $ q_*(s, a) = \cal R_s^a + \gamma \dsum_{s' \in \cal S} \cal P_{ss'}^a v_*(s') $.

- 代入即得

  - $ \d v_*(s) = \max_a \pqty{
    	\cal R_s^a + \gamma \dsum_{s' \in \cal S}
    	\cal P_{ss'}^a v_*(s')
    } $. (不包含最优行为值函数的方程)

  - $ q_*(s, a) = \cal R_s^a + \gamma \dsum_{s' \in \cal S} \cal P_{ss'}^a \max\limits_{a'} q_*(s', a') $. (不包含最优状态值函数的方程)

    <span style="border-left: 4px solid #dfe2e5; padding: 0 15px; color: #777777; padding-right: 0;">这两个公式最为重要.</span>

- 最优策略 $ \pi_* $ 即使得状态值函数最大的策略.

  <span style="border-left: 4px solid #dfe2e5; padding: 0 15px; color: #777777; padding-right: 0;">最优策略及以下表达式了解即可.</span>

  - $ v_{\pi_*}(s) = v_*(s) $.
  - $ q_{\pi_*}(s, a) = q_*(s, a) $.
  - $ \pi_*(a \mid s) = \bb I\Bqty{a = \argmax{a \in \cal A} q_*(s, a)} $.
  - 其中示性函数 $ \bb I\Bqty{A} $ 在 $A$ 发生 (或为真) 时为 1, 其余情况为 0.

