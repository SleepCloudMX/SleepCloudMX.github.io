<h1 align="center">强化学习</h1>

$$
% 设置
\newcommand{\aneg}[1]{\hspace{-0.75em}&#1&\hspace{-0.75em}}
\newcommand{\aneq}{\aneg{=}}
% 上述指令用于在使用 array 环境时调整等号左右间距
\newcommand{\noeq}{&\hspace{1.3em}}
% 上述指令用于 align 环境中, 类似与 &= 但不显示等号.
\renewcommand{\d}{\displaystyle}

% 字符
\renewcommand{\i}{\mathrm{i}}
\renewcommand{\j}{\mathrm{j}}
\renewcommand{\k}{\mathrm{k}}
\newcommand{\e}{\textup{e}}
\newcommand{\ve}{\varepsilon}
\newcommand{\Beta}{\mathrm{B}}
\newcommand{\omicron}{\mathit{o}}
\newcommand{\Omicron}{\mathit{O}}

% 原本的定义为:
% \newcommand{\cal}[1]{\mathcal{#1}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\renewcommand{\cal}[1]{\mathcal#1}
\renewcommand{\scr}[1]{\mathscr#1}
\renewcommand{\frak}[1]{\mathfrak#1}
\newcommand{\bb}[1]{\mathbb#1}

% 数集
\newcommand{\D}{\mathbb{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\J}{\mathbb{J}}
\newcommand{\K}{\mathbb{K}}
\renewcommand{\L}{\mathbb{L}}

% 上下标
\newcommand{\trans}{^\mathrm{T}}
\newcommand{\inv}{^{-1}}
\newcommand{\madj}[1]{^{\pqty{#1^*}}}	% m 重伴随矩阵
\newcommand{\adj}{^*}
\newcommand{\vector}[1]{\overrightarrow{#1}}
\newcommand{\wavy}[1]{\overset\sim#1}	% \tilde 或 \widetilde 不明显, 容易与 \bar 或 \overline 混淆

% 序列
\newcommand{\ccdots}{\cdot\cdots\cdot}
\newcommand{\oneton}{1,2,\cdots,n}
\newcommand{\oneto}[1]{1,2,\cdots,#1}

\newcommand{\ssto}[3]{#1_1 #3 #1_2 #3 \cdots #3 #1_{#2}}
\newcommand{\ssup}[3]{#1^1 #3 #1^2 #3 \cdots #3 #1^{#2}}
\newcommand{\soneto}[2]{\ssto{#1}{#2}{,}}
\newcommand{\splus}[2]{\ssto{#1}{#2}{+}}

% 括号
\newcommand{\aqty}[1]{\expval{#1}}
\newcommand{\pbqty}[1]{\left(#1\right]}
\newcommand{\bpqty}[1]{\left[#1\right)}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}

% 矩阵宏简写
\newcommand{\bmatrix}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\Bmatrix}[1]{\begin{Bmatrix}#1\end{Bmatrix}}
\newcommand{\vmatrix}[1]{\begin{vmatrix}#1\end{vmatrix}}
\newcommand{\Vmatrix}[1]{\begin{Vmatrix}#1\end{Vmatrix}}

% 常用微分
\newcommand{\dx}{\dd{x}}
\newcommand{\dy}{\dd{y}}
\newcommand{\dz}{\dd{z}}
\newcommand{\dt}{\dd{t}}
\newcommand{\ds}{\dd{s}}
\newcommand{\dr}{\dd{r}}

% 一般的微分
% 如果只使用 \dd{x}\dd{y} 的话, 中间会有多余的间隔.
\newcommand{\df}{\dd}
\newcommand{\ddf}[2]{\,\mathrm{d}#1\mathrm{d}#2}	% 微分形式 differential form
\newcommand{\dddf}[3]{\,\mathrm{d}#1\mathrm{d}#2\mathrm{d}#3}

% 高阶微分
\newcommand{\dxdy}{\ddf{x}{y}}
\newcommand{\dydz}{\ddf{y}{z}}
\newcommand{\dzdx}{\ddf{z}{x}}
\newcommand{\dudv}{\ddf{u}{v}}
\newcommand{\drdt}{\ddf{r}{\theta}}
\newcommand{\dxdydz}{\dddf{x}{y}{z}}

% 矩阵的宏指令
\newcommand{\pmcmn}[3]{\begin{pmatrix}
	#1_{11} & #1_{12} & \cdots & #1_{1#3} \\
	#1_{21} & #1_{22} & \cdots & #1_{n#3} \\
	\vdots & \vdots && \vdots \\
	#1_{#2 1} & #1_{#2 2} & \cdots & #2_{n#3} \\
\end{pmatrix}}

\newcommand{\pmc}[1]{\pmcmn{#1}{n}{n}}
\newcommand{\pvcn}[2]{\begin{pmatrix}
	#1_1 \\ #1_2 \\ \vdots \\ #1_{#2}
\end{pmatrix}}

\newcommand{\pvc}[1]{\pvcn{#1}{n}}
\newcommand{\pto}{\overset{P}{\to}}

% 函数名
\renewcommand{\char}{\operatorname{char}}	% 由于已存在此命令, 不可使用 DeclareMathOperator
\renewcommand{\r}{\operatorname{r}}
\DeclareMathOperator{\st}{s.t.\,}	% 虽然不是函数名, 但用了这个指令就放这儿了.
\DeclareMathOperator{\diag}{diag}	% 不需要定义太多, 一个文件里用到什么定义什么,
\DeclareMathOperator{\Ker}{Ker}		% 毕竟特殊的函数名太多太多了.
\DeclareMathOperator{\Aut}{Aut}		% 便捷与效率的权衡.
\DeclareMathOperator{\Inn}{Inn}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\stab}{stab}
\DeclareMathOperator{\orb}{orb}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\rot}{rot}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Outer}{Outer}
\DeclareMathOperator{\Even}{Even}
\DeclareMathOperator{\Scalar}{Scalar}
\DeclareMathOperator{\Vector}{Vector}
\DeclareMathOperator{\arsh}{arsh}
\DeclareMathOperator{\arch}{arch}
\DeclareMathOperator{\arth}{arth}
\renewcommand{\Re}{\operatorname{Re}}	% 自带 \Re 的效果是 \mathrm{Re}, 前后无空格, 故重写
\renewcommand{\Im}{\operatorname{Im}}
\DeclareMathOperator{\Sa}{Sa}
\DeclareMathOperator{\Si}{Si}

% 运算符
% 可以用 \bigcap, \bigcup, \bigoplus, \bigotimes 替代
\newcommand{\capop}{\displaystyle\mathop\cap\limits}
\newcommand{\cupop}{\displaystyle\mathop\cup\limits}
\newcommand{\oplusop}{\mathop\oplus\limits}
\newcommand{\otimesop}{\mathop\otimes\limits}
\newcommand{\bigoplusop}{\mathop\bigoplus\limits}
\newcommand{\bigotimesop}{\mathop\bigotimes\limits}

% 积分
\newcommand{\dint}{\displaystyle\int}
\newcommand{\inti}{\dint_{-\infty}^{+\infty}}
\newcommand{\intoi}{\dint_0^{+\infty}}

\newcommand{\intl}{\displaystyle\int\limits}
\newcommand{\iintl}{\displaystyle\iint\limits}
\newcommand{\iiintl}{\displaystyle\iiint\limits}

% 求和
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\csum}[1]{\dsum_{#1=1}^\infty}
\newcommand{\nsum}{\csum{n}}
\newcommand{\nsuminf}{\dsum_{n=-\infty}^{+\infty}}
\newcommand{\ksum}{\csum{k}}
\newcommand{\nosum}{\dsum_{n=0}^\infty}
\newcommand{\insum}{\dsum_{i=1}^n}
\newcommand{\knsum}{\dsum_{k=1}^n}

% 求积
\newcommand{\dprod}{\displaystyle\prod}
\newcommand{\nprod}{\dprod_{n=1}^\infty}
\newcommand{\noprod}{\dprod_{n=0}^\infty}
\newcommand{\inprod}{\dprod_{i=1}^n}

% 极限
\newcommand{\liml}{\lim\limits}
\newcommand{\ulim}{\overline\lim\limits_{n\to\infty}}
\newcommand{\dlim}{\underline\lim\limits_{n\to\infty}}
% 注意这里的 d 是 down, 而不是 displaystyle

\newcommand{\xlim}{\lim\limits_{x\to x_0}}
\newcommand{\nlim}{\lim\limits_{n\to\infty}}
\newcommand{\clim}[1]{\lim\limits_{#1\to\infty}}

% 并集
\newcommand{\incup}{\bigcup_{i=1}^n}
\newcommand{\ncup}{\bigcup_{n=1}^\infty}
\newcommand{\icup}{\bigcup_{i=1}^\infty}

% 交集
\newcommand{\incap}{\bigcap_{i=1}^n}
\newcommand{\ncap}{\bigcap_{n=1}^\infty}
\newcommand{\icap}{\bigcap_{i=1}^\infty}

% 差分
\newcommand{\DD}{\Delta}
\newcommand{\DV}[2]{\dfrac{\DD#1}{\DD#2}}
\newcommand{\nDV}[3]{\dfrac{\DD^{#1}#2}{\DD#3^{#1}}}

% 求导
\newcommand{\ddv}{\displaystyle\dv}
\newcommand{\dpdv}{\displaystyle\pdv}

% 最值 (返回参数); 暂时先这么凑合着用吧
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}}

% 缩写
\newcommand{\LRA}{\Leftrightarrow}
\newcommand{\RLA}{\Leftrightarrow}
\newcommand{\LA}{\Leftarrow}
\newcommand{\RA}{\Rightarrow}

\newcommand{\lra}{\leftrightarrow}
\newcommand{\rla}{\leftrightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\ra}{\rightarrow}

\newcommand{\QRLA}{\quad\RLA\quad}
\newcommand{\QRA}{\quad\RA\quad}
\newcommand{\LLRA}{\Longleftrightarrow}

\newcommand{\QNRA}{\quad\nRightarrow\quad}
\newcommand{\qnra}{\quad\nrightarrow\quad}

\newcommand{\wt}{\widetilde}

% 图形符号
\newcommand{\qed}{\quad\square}
\renewcommand{\parallel}{\mathrel{/\mskip-2.5mu/}}
\newcommand{\paralleleq}{\hspace{0.5em}{^{^{\parallel}}}\hspace{-1.04em}=}
\newcommand{\rt}{\matrm{Rt}\triangle}

% 分块矩阵
\newenvironment{mat}[1]{
	\begin{array}{#1}
}{
	\end{array}
}

\newenvironment{pmat}[1]{
	\left( \begin{array}{#1}
}{
	\end{array} \right)
}

\newenvironment{bmat}[1]{
	\left[ \begin{array}{#1}
}{
	\end{array} \right]
}

\newenvironment{Bmat}[1]{
	\left\{ \begin{array}{#1}
}{
	\end{array} \right\}
}

\newenvironment{vmat}[1]{
	\left\lvert \begin{array}{#1}
}{
	\end{array} \right\rvert
}

\newenvironment{Vmat}[1]{
	\left\lVert \begin{array}{#1}
}{
	\end{array} \right\rVert
}
$$

[TOC]

### 2.1  强化学习

#### 2.1.1  强化学习的基本原理

- 强化学习的原理

  - 智能体根据当前的状态 $ s_t $ 及其回报 $ r_t $，做出行为 $ a_t $.（这里不研究与之前状态有关的情况）

    智能体又称为 RL agent，即 Reinforcement Learning agent.

  - 环境在收到状态 $ s_t $ 下智能体的行为 $ a_t $ 后，将智能体转移到下一状态 $ s_{t+1} $，并提供回报 $ r_{t+1} $.

    <img src="image/2.1.2 强化学习原理.png" alt="image-20230503152704678" style="zoom:75%;" />

  - 该过程循环进行，直到抵达终止条件.

  - 强化学习的模型是<u>马尔科夫决策过程</u>.

  - 我们将学习强化学习中三类常见的算法：<u>动态规划</u>、<u>时序差分</u>、<u>蒙特卡洛</u>.

- 智能体的组成

  - 策略
    - $ \pi $ 是一个策略 (Policy).
    - $ \pi(s) $ 表示状态 $ s $ 下可能发生的行为, 是一个随机变量.
    - $ \pi(a \mid s) = P(A_t = a \mid S_t = s) $.
  - 值函数
    - $t$ 时刻的状态为 $ S_t $, 立即回报为 $ R_t $.
    - 整体回报为 $ G_t = \dsum_{k=0}^\infty \gamma^k R_{t + k + 1} $, 又称为长期回报.
    - 状态值函数 (价值, Value) $ V_\pi(s) = E_\pi\bqty{G_t \mid S_t = s} $.
    - 行为值函数 $ Q_\pi(s, a) = E_\pi\bqty{G_t \mid S_t = s, A_t = a} $.
  - 模型
    - 状态转移概率 $ P_{ij}^\pi = P\Bqty{S_{t + 1} = s' \mid S_t = s} $.
    - 状态转移概率 $ P_{ij}^a = P\Bqty{S_{t + 1} = s' \mid S_t = s, A_t = a} $.
    - "状态期望立即回报" $ R_s^\pi = E\bqty{R_{t + 1} \mid S_t = s} $.
    - "行为期望立即回报" $ R_s^a = E\bqty{R_{t+1} \mid S_t = s, A_t = a} $.



#### 2.1.2  强化学习的三对概念

- 学习与规划
  - 学习：智能体通过与环境交互的过程，以此估计环境模型的参数，或者调整智能体行为.
  - 规划：根据学习得到的数据，优化智能体的策略，从而得到最大的回报的过程.
- 探索与利用
  - 利用：根据采取样本的信息，选取当下局部最优的行为.
  - 探索：不仅仅采取当下最优的行为，而是探索新的行为，以期得到全局最优的行为.
- 预测与控制
  - 预测：评估当前策略，即计算或估计状态值函数或行为值函数.
  - 控制：根据对当前策略评估而得到的值函数，对策略进行优化.



#### 2.1.3  强化学习的具体分类（略）

<span style="border-left: 4px solid #dfe2e5; padding: 0 15px; color: #777777; padding-right: 0;">这部分可略去不看.</span>

- 机器学习 (属于人工智能)

  - 分类一
    - 监督学习
      - 有标记的数据.
      - 预测未知数据的标记.
      - 静态数据.
    - 非监督学习
      - 无标记的数据.
      - 挖掘数据潜在结构.
      - 静态数据.
    - 强化学习
      - 没有标记, 只有一个延迟的回报信号.
      - 属于序贯决策 (Sequential Decision Making) 模型.
      - 数据通过与环境不断交互而产生, 即动态数据.
      - 数据之间高度相关.
  - 分类二
    - 传统的机器学习: 需要人工提取特征.
    - 深度学习: 无需人工提取特征 (属于监督学习).
    - 强化学习: 目的是在环境中最大化奖励.
    - 深度强化学习: 结合深度学习与强化学习.

- 强化学习

  - 分类一
    - 有模型方法: 如动态规划法.
    - 无模型方法: 如蒙特卡洛法、时序差分法.
  - 分类二
    - 基于值函数的方法 (Value Based)
    - 基于策略的方法 (Policy Based)
    - 行动者-评论家方法 (Actor-Critic)



### 2.2  动态规划

#### 2.2.1  思路介绍

我们想要求解强化学习模型（即马尔科夫决策过程）的最优策略，可以循环进行策略评估与策略提升：

- 策略评估（预测）：计算当前策略的状态值函数或行为值函数.
- 策略提升（控制）：根据当前策略的值函数去优化策略.

反复进行上述过程，直到策略稳定为最优策略. 该思路称为广义策略迭代.

动态规划、蒙特卡洛、时序差分，都属于广义策略迭代，其中动态规划需要知道模型的参数（如回报函数与状态转移概率矩阵），蒙特卡洛与时序差分则无需模型参数.

动态规划（DP，dynamic planning）分为策略迭代和价值迭代两种算法.



#### 2.2.2  策略迭代

##### 1  策略评估

在马尔科夫决策过程中，我们得到了贝尔曼期望方程，
$$
\begin{align}
v_\pi(s) &= \dsum_{a \in \cal A} \pi(a \mid s) \pqty{
	\cal R_s^a + \gamma \dsum_{s' \in \cal S}
	\cal P_{ss'}^a v_\pi(s')
}, \\
q_\pi(s, a) &= \cal R_s^a +
\gamma \dsum_{s' \in \cal S} \cal P_{ss'}^a
\dsum_{a' \in \cal A} \pi(a' \mid s') q_\pi(s', a').
\end{align}
$$
我们可以直接联立方程去求解，但是这样做计算量很大，实际应用不便.

另一种思路是，利用上述方程自举求得近似值，逐渐逼近精确值：
$$
\begin{align}
v^{(k+1)}_\pi(s) &= \dsum_{a \in \cal A} \pi(a \mid s) \pqty{
	\cal R_s^a + \gamma \dsum_{s' \in \cal S}
	\cal P_{ss'}^a v^{(k)}_\pi(s')
}, \\
q^{(k+1)}_\pi(s, a) &= \cal R_s^a +
\gamma \dsum_{s' \in \cal S} \cal P_{ss'}^a
\dsum_{a' \in \cal A} \pi(a' \mid s') q^{(k)}_\pi(s', a').
\end{align}
$$
实际应用中只需求解行为值函数. 如果采取任一行为后状态的转移是确定的, 而非随机的, 则可以不求解行为值函数, 而转为求解状态值函数.

##### 2  策略改进

记改进前的策略为 $ \pi_n $, 则改进后的策略为 $ \pi_{n+1}(s) \in \argmax a Q_{\pi_n}(s, a) $.



#### 2.2.3  价值迭代

##### 1  算法介绍

利用贝尔曼最优方程自举,
$$
V_{k + 1}(s) = \max_{a \in A} \pqty{
	R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a V_k(s')
},
$$
求出最优值函数后, 贪心策略即为最优策略.

##### 2  算法优势

策略迭代中，每次迭代都要通过自举进行策略评估；而价值迭代，只需要自举求得最优值函数. 因此一般来说，价值迭代的计算量更小.



### 2.3  蒙特卡洛

#### 2.3.1  蒙特卡洛思路介绍

动态规划在策略评估时，需要知道模型的全部参数（状态转移概率矩阵与回报函数），但实际情景中不一定可知，即使可知，也可能十分复杂. 因此我们通过采样数据去估计值函数，该思路称为蒙特卡洛方法（MC，Monte-Carlo）.



#### 2.3.2  在线策略蒙特卡洛

##### 1  蒙特卡洛评估

- 采样得到轨迹

  - 立即回报: $ \aqty{s_0, a_0, r_1, \cdots, s_T, a_T, r_T} $.（直接得到的数据，不便于使用）
  - 累积回报: $ \aqty{s_0, a_0, G_1, \cdots, s_T, a_T, G_T} $.（由立即回报计算而得，便于使用）

- 计算平均回报

  - 初访法：只考虑每个轨迹中第一次到访状态 $s$ 时的累积回报.（可能会有偏差）
  - 每访法：考虑每个轨迹中每一次到访状态 $s$ 时的累积回报.（无偏估计）

- 增量式公式

  - 设第 $k$ 个采样数据估计出的值函数为 $ V_{k+1}(s_t) $.

  - 则得到第 $ k + 1 $ 个采样数据 $ G_t $ 后的值函数为
    $$
    \begin{align}
    V_{k+1}(s_t)
    &\la \dfrac{
    	k V_k(s_t) + G_t
    }{k + 1}
    = V_k(s_t) + \dfrac{
    	G_t - V_k(s_t)
    }{k + 1}.
    \end{align}
    $$

- 修正后的更新公式

  - 若认为越靠后的累积回报越重要, 可将 $ \dfrac{1}{k+1} $ 替换为 $ \alpha \in (0, 1) $.

  - $\alpha$ 表示更新步长, 可以是 $k$ 的函数. $\alpha$ 越大, 代表越靠后的累积回报越重要.
    $$
    V(s_t) \ra V(s_t) + \alpha(G_t - V(s_t)).
    $$

- 若要估计行为值函数，则类似可得
  $$
  Q(s_t, a_t) \la
  Q(s_t, a_t) + \alpha(G_t - Q(s_t, a_t)).
  $$

##### 2  蒙特卡洛控制

由于采样数据是有限的，不一定能反映全局的最优解，因此我们使用 $ \ve $-贪心探索，即

- 以极小的概率 $ \ve $ 从所有行为中均匀随机选取一个.
- 以 $ 1 - \ve $ 的概率选取当前最优行为.

换言之，若一共有 $m$ 个行为, 有且仅有一个最优行为, 那么

- 采取最优行为的概率为 $ 1 - \dfrac{m-1}{m} \ve $.
- 采取其它任一行为的概率为 $ \dfrac{\ve}{m} $.

##### 3  在线/离线策略

首先引入概念：

- 行为策略：即产生采样数据的策略. 如果我们要估计状态 $s$ 的值函数，那么采样至状态 $s$ 时使用的策略就是行为策略.
- 原始策略（目标策略）：被评估改进的策略. 由行为策略到达状态 $s$ 之后，不断采取行为、转移状态、最终获取整体回报的过程中使用的策略就是原始策略.

上述蒙特卡洛方法中，行为策略与原始策略相同，都是 $ \ve $-贪心策略，称为 **在线策略蒙特卡洛**.

但是最终我们想得到的，是一个确定性的而非随机性的策略，因此希望通过 $ \ve $-贪心策略获取更丰富的采样数据，通过贪心策略得到整体回报，那么最终我们得到的策略（贪心策略）就是确定性的了.

像这样在线策略与原始策略不同的蒙特卡洛方法，称为 **离线策略蒙特卡洛**，也就是下一小节中所要探讨的.



#### 2.3.3  离线策略蒙特卡洛（略）

<span style="border-left: 4px solid #dfe2e5; padding: 0 15px; color: #777777; padding-right: 0;">可以跳过本节不看.</span>

- 在策略评估时 (即行为策略 $ \pi' $): $\ve$-贪心策略或随机策略.
- 在策略改进时 (即原始策略 $ \pi $): 非 $ \ve $-贪心策略 (如贪心策略).

##### 1  重要性采样方法

- 前置知识
  - $ E\bqty{X} = E\bqty{E\bqty{X \mid Y}} $.
  - $ E\bqty{f(x)} = \dint_x p(x) f(x) \dx = \dint_x q(x) \dfrac{p(x)}{q(x)} f(x) \dx $.
  - $ \hat E\bqty{f(x)} = \dfrac{1}{m} \dsum_{i=1}^m f(x_i) = \dfrac{1}{m} \dsum_{i=1}^m \dfrac{p(x'_i)}{q(x'_i)} f(x'_i) $.
  - 随机采样, 可得期望的无偏估计.
- 行为值函数
  - 使用策略 $ \pi' $: $ Q(s, a) = \dfrac{1}{m} \dsum_{i = 1}^m \dfrac{P_i^\pi}{P_i^{\pi'}} G_i $.
  - **重要采样比率** $ \rho_i^T = \dfrac{P_i^\pi}{P_i^{\pi'}} = \d\prod_{j=0}^{T-1} \dfrac{\pi(a_j \mid s_j)}{\pi'(a_j \mid s_j)} $.
  - 增量更新公式 $ Q(s_t, a_t) \la Q(s_t, a_t) + \alpha(\rho_i^T G_t - Q(s_t, a_t)) $.



##### 2  加权重要性采样

为减小方差,
$$
\begin{align}
\hat E\bqty{f(x)} &= \dfrac{
	\dsum_{i=1}^m \dfrac{p(x'_i)}{q(x'_i)} f(x'_i)
}{
	\dsum_{i=1}^m \dfrac{p(x'_i)}{q(x'_i)}
}, \quad
Q(s, a) = \dfrac{
	\dsum_{i=1}^m \rho_i^T G_i
}{
	\dsum_{i=1}^m \rho_i^T
}, \\
Q_{m}(s, a) &= \dfrac{
	\rho_{m}^T G_{m} + Q_{m-1}(s, a) \dsum_{i=1}^{m-1} \rho_i^T
}{
	\dsum_{i=1}^m \rho_i^T
} \\
&= Q_{m-1}(s, a) + \dfrac{\rho_m^T}{
	\dsum_{i=1}^m \rho_i^T
} \pqty{
	G_m - Q_{m-1}(s, a)
}.
\end{align}
$$



### 2.4  时序差分

#### 2.4.1  时序差分思路介绍

##### 1  时序差分简介

在蒙特卡洛方法中，每次采样都需要得到完整的轨迹，只有这样才能计算出整体回报 $ G_t $，从而估计值函数 $ V_\pi(S_t) = E_\pi\bqty{G_t \mid S_t = s} $.

而由贝尔曼期望方程，$ V_\pi(S_t) = E_\pi\bqty{
	R_{t + 1} + \gamma V(S_{t + 1}) \mid S_t = s
} $，状态值函数 $ V_\pi(S_t) $ 不仅是整体回报 $ G_t $ 的条件期望，还是 $ R_{t+1} + \gamma V(S_{t+1}) $ 的期望，因此我们可以采样算出其均值，以估计值函数.

这么做的好处是，只需要一部分的轨迹，从而缩短了采样的时间，从而更快地估计值函数.

这个思路称为时序差分（TD, Temporal Difference），其中替代 $ G_t $ 的 $ R_{t + 1} + \gamma V(S_{t + 1}) $ 称为 **TD 目标值**，它的条件期望就是状态值函数，它与状态值函数之差 $ \delta_t = R_{t + 1} + \gamma V(S_{t + 1}) - V(s_t) $ 称为 **TD 误差**.

时序差分与蒙特卡洛都是无模型方法，同样分为在线策略（如 Sarsa）与离线策略（如 Q-Learning）两种.



##### 2  三种算法对比（略）

- 动态规划 (DP, Dynamic Programming)
  - $ V_\pi(S_t) = E_\pi\bqty{R_{t + 1} + \gamma V(S_{t+1}) \mid S_t = s} $.
  - 一步预测, 自举. 无需采样, 需要完整模型.
  - 无偏差, 无方差.
  - 有模型方法, 具有马尔科夫性.
- 蒙特卡洛 (MC, Monte Carlo)
  - $ V_\pi(S_t) \approx G_t \mid S_t = s $.
  - 不自举. 依靠采样, 学习完整的轨迹.
  - 无偏估计, 方差较大.
  - 无模型方法, 无马尔科夫性.
- 时序差分 (TD, Temporal Difference)
  - $ V_\pi(S_t) \approx R_{t + 1} + \gamma V(S_{t + 1}) \mid S_t = s $.
  - 一步预测, 自举. 需要采样, 学习部分轨迹.
  - 右偏估计, 方差较小.
  - 无模型方法, 无马尔可夫性.

三种算法都遵循广义策略迭代框架.



#### 2.4.2  在线策略时序差分：Sarsa

$$
Q(S, A) \la Q(S, A) + \alpha \pqty{
	R + \gamma Q(S', A') - Q(S, A)
}.
$$

其中行为 $A$ 由行为策略得到，行为 $ A' $ 由目标策略得到.

这里的行为策略与目标策略均为 $ \ve $-贪心策略.



#### 2.4.3  离线策略时序差分：Q-Learning

##### 1  离线策略 TD（略）

- 离线策略 TD

  - 由行为策略 $ \mu(a \mid s) $ 进行数据采样.
  - 评估和改进原始策略 $ \pi(a, s) $.

- TD 目标

  - 使用原始策略 $ \pi $ 评估策略 $ \pi $, 则为 $ R_{t + 1} + \gamma Q(s_{t + 1}, a_{t + 1}) $.
  - 使用行为策略 $ \mu $ 评估策略 $ \pi $, 则为 $ \dfrac{\pi(a_t \mid s_t)}{\mu(a_t \mid s_t)} \pqty{R_{t + 1} + \gamma Q(s_{t + 1}, a_{t + 1})} $.

- 离线策略 TD 方法的更新公式
  $$
  Q(s_t, a_t) \la
  Q(s_t, a_t) + \alpha \pqty{
  	\dfrac{
  		\pi(a_t \mid s_t)
  	}{
  		\mu(a_t \mid s_t)
  	} \pqty{
  		R_{t + 1} + \gamma Q(s_{t + 1}, a_{t + 1})
  	} - Q(s_t, a_t)
  }.
  $$




##### 2  Q-learning

$$
Q(S_t, A_t) \la
Q(S_t, A_t) + \alpha \pqty{
	R_{t + 1} + \gamma Q(S_{t + 1}, A') - Q(S_t, A_t)
}.
$$

- 其中 $ A_t $ 由行为策略 $ \mu $ 产生. (如 $ \ve $-贪心策略).

- TD 目标中的行为 $A'$ 由目标策略 $ \pi $ (贪心策略) 产生, 即
  $$
  A' = \pi(S_{t + 1}) = \argmax{a'} Q(S_{t + 1}, a'),
  $$

- 于是 TD 目标可写为 $ R_{t + 1} + \gamma \max_{a'} Q(S_{t + 1}, a') $, 上式变为
  $$
  Q(S_t, A_t) \la Q(S_t, A_t) + \alpha \pqty{
  	R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)
  }.
  $$

- 备注：一般来说，离线策略产生的轨迹数据更为丰富，且获得的结果是确定性策略，因此比较常用.

